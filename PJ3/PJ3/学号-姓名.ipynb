{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 BiLSTM-based NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CHisIECDataset(Dataset):\n",
    "    label_label_id_mapping = {\n",
    "        \"O\": 0,\n",
    "        \"B-PER\": 1,\n",
    "        \"I-PER\": 2,\n",
    "        \"E-PER\": 3,\n",
    "        \"S-PER\": 4,\n",
    "        \"B-LOC\": 5,\n",
    "        \"I-LOC\": 6,\n",
    "        \"E-LOC\": 7,\n",
    "        \"S-LOC\": 8,\n",
    "        \"B-OFI\": 9,\n",
    "        \"I-OFI\": 10,\n",
    "        \"E-OFI\": 11,\n",
    "        \"S-OFI\": 12,\n",
    "        \"B-BOOK\": 13,\n",
    "        \"I-BOOK\": 14,\n",
    "        \"E-BOOK\": 15,\n",
    "        \"S-BOOK\": 16,\n",
    "    }\n",
    "\n",
    "    def __init__(self, path) -> None:\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            d = [[], []]\n",
    "            while line := f.readline():\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    word, label = line.split()\n",
    "                    d[0].append(word)\n",
    "                    d[1].append(self.label_label_id_mapping[label])\n",
    "                elif d[0]:\n",
    "                    self.data.append(tuple(d))\n",
    "                    d = [[], []]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "\n",
    "def get_dataloader(dataset, shuffle=True):\n",
    "    def collect_fn(batch):\n",
    "        t = batch[0][0]\n",
    "        l = one_hot(torch.tensor(batch[0][1], dtype=torch.int64), 17).float()\n",
    "        return t, l\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        shuffle=shuffle,\n",
    "        batch_size=1,\n",
    "        collate_fn=collect_fn,\n",
    "    )\n",
    "\n",
    "\n",
    "train_set = CHisIECDataset(\"./CHisIEC/train.txt\")\n",
    "dev_set = CHisIECDataset(\"./CHisIEC/dev.txt\")\n",
    "test_set = CHisIECDataset(\"./CHisIEC/test.txt\")\n",
    "\n",
    "train_loader = get_dataloader(train_set)\n",
    "val_loader = get_dataloader(dev_set, shuffle=False)\n",
    "test_loader = get_dataloader(test_set, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "from torch import nn\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "\n",
    "from torch.nn import LSTM\n",
    "import torch\n",
    "\n",
    "\n",
    "class MyAwesomeModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim=50, hidden_dim=50) -> None:\n",
    "        super().__init__()\n",
    "        self.vectors = Vectors(\n",
    "            name=\"gigaword_chn.all.a2b.uni.ite50.vec\",\n",
    "            cache=\".\",\n",
    "        )\n",
    "        self.lstm = LSTM(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, 17)\n",
    "\n",
    "    def forward(self, x: str):\n",
    "        x = self.vectors.get_vecs_by_tokens(x).to(\"cuda\")\n",
    "        x, _ = self.lstm(x.unsqueeze(0))\n",
    "        x = self.classifier(x[0])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "model = MyAwesomeModel().cuda()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train(loader):\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    for x, y in loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        label = y.to(\"cuda\")\n",
    "        try:\n",
    "            loss = loss_fn(pred, label)\n",
    "        except:\n",
    "            print(pred.shape, label.shape)\n",
    "        epoch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return {\"loss\": sum(epoch_loss) / len(epoch_loss)}\n",
    "\n",
    "\n",
    "def eval(loader):\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    target = []\n",
    "    for x, y in loader:\n",
    "        _pred = model(x).argmax(-1)\n",
    "        pred += _pred.tolist()\n",
    "        _target = y.argmax(-1)\n",
    "        target += _target.tolist()\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(target, pred),\n",
    "        \"f1_macro\": f1_score(target, pred, average=\"macro\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "for epoch in trange(5, desc=\"Epoch\"):\n",
    "    metrics = train(train_loader)\n",
    "    with torch.no_grad():\n",
    "        metrics = {**eval(val_loader), **metrics}\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your conclusion in this section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
